[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1065][         clip_model_load] clip_model_load: loaded meta data with 18 key-value pairs and 377 tensors from ./models/llava/mmproj-model-f16.gguf
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1075][         clip_model_load] clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   0:                       general.architecture str              = clip
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   4:                          general.file_type u32              = 1
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   7:                     clip.vision.image_size u32              = 336
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  14:                    clip.vision.block_count u32              = 23
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  17:                              clip.use_gelu bool             = false
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f32:  235 tensors
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f16:  142 tensors
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1162][         clip_model_load] clip_model_load: CLIP using CPU backend
[1726383007] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1206][         clip_model_load] clip_model_load: params backend buffer size =  595.49 MB (377 tensors)
[1726383012] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_grid_pinpoints not found in file
[1726383012] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.mm_patch_merge_type not found in file
[1726383012] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_crop_resolution not found in file
[1726383012] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1504][         clip_model_load] clip_model_load: compute allocated memory: 32.89 MB
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1065][         clip_model_load] clip_model_load: loaded meta data with 18 key-value pairs and 377 tensors from ./models/llava/mmproj-model-f16.gguf
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1075][         clip_model_load] clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   0:                       general.architecture str              = clip
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   4:                          general.file_type u32              = 1
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   7:                     clip.vision.image_size u32              = 336
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  14:                    clip.vision.block_count u32              = 23
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  17:                              clip.use_gelu bool             = false
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f32:  235 tensors
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f16:  142 tensors
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1162][         clip_model_load] clip_model_load: CLIP using CPU backend
[1726383150] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1206][         clip_model_load] clip_model_load: params backend buffer size =  595.49 MB (377 tensors)
[1726383156] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_grid_pinpoints not found in file
[1726383156] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.mm_patch_merge_type not found in file
[1726383156] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_crop_resolution not found in file
[1726383156] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1504][         clip_model_load] clip_model_load: compute allocated memory: 32.89 MB
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1065][         clip_model_load] clip_model_load: loaded meta data with 18 key-value pairs and 377 tensors from ./models/llava/mmproj-model-f16.gguf
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1075][         clip_model_load] clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   0:                       general.architecture str              = clip
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   4:                          general.file_type u32              = 1
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   7:                     clip.vision.image_size u32              = 336
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  14:                    clip.vision.block_count u32              = 23
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  17:                              clip.use_gelu bool             = false
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f32:  235 tensors
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f16:  142 tensors
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1162][         clip_model_load] clip_model_load: CLIP using CPU backend
[1726383170] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1206][         clip_model_load] clip_model_load: params backend buffer size =  595.49 MB (377 tensors)
[1726383174] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_grid_pinpoints not found in file
[1726383174] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.mm_patch_merge_type not found in file
[1726383174] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_crop_resolution not found in file
[1726383174] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1504][         clip_model_load] clip_model_load: compute allocated memory: 32.89 MB
[1726383212] encode_image_with_clip: image embedding created: 576 tokens
[1726383212] 
encode_image_with_clip: image encoded in 11372.35 ms by CLIP (   19.74 ms per image patch)
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1065][         clip_model_load] clip_model_load: loaded meta data with 18 key-value pairs and 377 tensors from ./models/llava/mmproj-model-f16.gguf
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1075][         clip_model_load] clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   0:                       general.architecture str              = clip
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   4:                          general.file_type u32              = 1
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   7:                     clip.vision.image_size u32              = 336
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  14:                    clip.vision.block_count u32              = 23
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1091][         clip_model_load] clip_model_load: - kv  17:                              clip.use_gelu bool             = false
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f32:  235 tensors
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1100][         clip_model_load] clip_model_load: - type  f16:  142 tensors
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1162][         clip_model_load] clip_model_load: CLIP using CPU backend
[1726383231] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1206][         clip_model_load] clip_model_load: params backend buffer size =  595.49 MB (377 tensors)
[1726383238] encode_image_with_clip: image embedding created: 576 tokens
[1726383238] 
encode_image_with_clip: image encoded in 21207.15 ms by CLIP (   36.82 ms per image patch)
[1726383292] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_grid_pinpoints not found in file
[1726383292] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.mm_patch_merge_type not found in file
[1726383292] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp:  168][             get_key_idx] key clip.vision.image_crop_resolution not found in file
[1726383292] [C:\Users\joozk\AppData\Local\Temp\pip-install-4xqgjq2w\llama-cpp-python_1ea7c222d44141f2afc95c75e99bbebe\vendor\llama.cpp\examples\llava\clip.cpp: 1504][         clip_model_load] clip_model_load: compute allocated memory: 32.89 MB
[1726383407] encode_image_with_clip: image embedding created: 576 tokens
[1726383407] 
encode_image_with_clip: image encoded in 45003.04 ms by CLIP (   78.13 ms per image patch)
